const e="pytest",t=[{name:"File or Directory",description:"The test file or directory you want to run pytest on. If omitted, pytest will run all       of the files of the form test_*.py or *_test.py in the current directory       and its subdirectories",template:["filepaths","folders"],isOptional:!0}],i=[{description:"Control assertion debugging tools. 'plain' performs no assertion debugging. 'rewrite' (the default) rewrites assert statements in test modules on import to provide assert expression information",name:"--assert",args:[{name:"Mode",suggestions:["plain","rewrite"]}]},{description:"Base temporary directory for this test run.(warning: this directory is removed if it exists)",name:"--basetemp",args:[{name:"Directory",template:["folders"]}]},{description:"Load configuration from `file` instead of trying to locate one of the implicit configuration files",name:"-c",args:[{name:"File",template:["filepaths"]}]},{description:"Remove all cache contents at start of test run",name:"--cache-clear"},{description:"Show cache contents, don't perform collection or tests. Optional argument: glob (default: '*')",name:"--cache-show",args:[{name:"Glob",isOptional:!0}]},{description:"Per-test capturing method",name:"--capture",args:[{name:"Method",description:"One of fd|sys|no|tee-sys",suggestions:["fd","sys","no","tee-sys"]}]},{description:"Whether code should be highlighted (only if --color is also enabled)",name:"--code-highlight",args:[{name:"Highlight",suggestions:["yes","no"]}]},{description:"Only collect tests, don't execute them",name:["--co","--collect-only"]},{description:"Don't ignore tests in a local virtualenv directory",name:"--collect-in-virtualenv"},{description:"Color terminal output",name:"--color",args:[{name:"Color",suggestions:["yes","no","auto"]}]},{description:"Only load conftest.py's relative to specified dir",name:"--confcutdir",args:[{name:"Dir",template:["folders"]}]},{description:"Force test execution even if collection errors occur",name:"--continue-on-collection-errors"},{description:"Store internal tracing debug information in this log file. This file is opened with 'w' and truncated as a result, care advised. Defaults to 'pytestdebug.log'",name:"--debug",args:[{name:"Debug File Name",isOptional:!0}]},{description:"Show N slowest setup/test durations (N=0 for all)",name:"--durations",args:[{name:"N",description:"(N=0 for all)"}]},{description:"Minimal duration in seconds for inclusion in slowest list",name:"--durations-min",args:[{name:"N"}]},{description:"Deselect item (via node id prefix) during collection (multi-allowed)",name:"--deselect",args:[{name:"nodeid_prefix"}]},{description:"Disable warnings summary",name:["--disable-warnings","--disable-pytest-warnings"]},{description:"For a given doctest, continue to run after the first failure",name:"--doctest-continue-on-failure"},{description:"Ignore doctest ImportErrors",name:"--doctest-ignore-import-errors"},{description:"Run doctests in all .py modules",name:"--doctest-modules"},{description:"Choose another output format for diffs on doctest failure",name:"--doctest-report",args:[{name:"Output format",description:"None,cdiff,ndiff,udiff,only_first_failure",suggestions:["none","cdiff","ndiff","udiff","only_first_failure"]}]},{description:"Doctests file matching pattern, default: test*.txt",name:"--doctest-glob",args:[{name:"Pattern"}]},{description:"Exit instantly on first error or failed test",name:["--exitfirst","-x"]},{description:"Run all tests, but run the last failures first",name:["--failed-first","--ff"]},{description:"Shows builtin and custom fixtures. Note that this command omits fixtures with leading _ unless the -v option is added",name:"--fixtures"},{description:"Show fixtures per test",name:"--fixtures-per-test"},{description:"Don't cut any tracebacks (default is to cut)",name:"--full-trace"},{description:"This shows help on command line and config-line options",name:["--help","-h"]},{description:"Ignore path during collection (multi-allowed)",name:"--ignore",args:[{name:"Path",template:["filepaths"]}]},{description:"Ignore path pattern during collection (multi-allowed)",name:"--ignore-glob",args:[{name:"Path",template:["filepaths"]}]},{description:"Prepend/append to sys.path when importing test modules and conftest files, default is to prepend",name:"--import-mode",args:[{name:"Mode",suggestions:["prepend","append","importlib"]}]},{description:"Create junit-xml style report file at given path",name:"--junit-xml",args:[{name:"Path",template:["filepaths","folders"]}]},{description:"Prepend prefix to classnames in junit-xml output",name:"--junit-prefix",args:[{name:"Str",description:"String to prepend"}]},{description:"Only run tests which match the given substring expression. An expression is a python evaluatable expression where all names are substring-matched against test names and their parent classes. Example: -k 'test_method or test_other' matches all test functions and classes whose name contains 'test_method' or 'test_other', while -k 'not test_method' matches those that don't contain 'test_method' in their names. -k 'not test_method and not test_other' will eliminate the matches. Additionally keywords are matched to classes and functions containing extra names in their 'extra_keyword_matches' set, as well as functions which have names assigned directly to them. The matching is case- insensitive",name:"-k",args:[{name:"Expression",description:"Ex: 'test_method or test_other'"}]},{description:"Keep duplicate tests",name:"--keep-duplicates"},{description:"Show locals in tracebacks (disabled by default)",name:["--showlocals","-l"]},{description:"Which tests to run with no previously (known) failures",name:["--last-failed-no-failures","--lfnf"],args:[{name:"Tests",suggestions:["all","none"]}]},{description:"Rerun only the tests that failed at the last run (or all if none failed)",name:["--last-failed","--lf"]},{description:"Auto-indent multiline messages passed to the logging module. Accepts true|on, false|off or an integer",name:"--log-auto-indent",args:[{name:"Log Auto Indent Setting",suggestions:["true","false"]}]},{description:"Cli logging level",name:"--log-cli-level",args:[{name:"Log CLI Level",suggestions:["CRITICAL","ERROR","WARNING","INFO","DEBUG"]}]},{description:"Log format as used by the logging module",name:"--log-cli-format",args:[{name:"Log CLI Format"}]},{description:"Log date format as used by the logging module",name:"--log-cli-date-format",args:[{name:"Log CLI Date Format"}]},{description:"Log date format as used by the logging module",name:"--log-date-format",args:[{name:"Log Date Format"}]},{description:"Log format as used by the logging module",name:"--log-format",args:[{name:"Log Format"}]},{description:"Path to a file where logging will be written to",name:"--log-file",args:[{name:"Log File Path",template:["filepaths"]}]},{description:"Log file logging level",name:"--log-file-level",args:[{name:"Log File Level",suggestions:["CRITICAL","ERROR","WARNING","INFO","DEBUG"]}]},{description:"Log date format as used by the logging module",name:"--log-file-date-format",args:[{name:"Log File Date Format"}]},{description:"Log format as used by the logging module",name:"--log-file-format",args:[{name:"Log File Format"}]},{description:"Level of messages to catch/display. Not set by default, so it depends on the root/parent log handler's effective level, where it is `WARNING` by default",name:"--log-level",args:[{name:"Level",suggestions:["CRITICAL","ERROR","WARNING","INFO","DEBUG"]}]},{description:"Only run tests matching given mark expression",name:"-m",args:[{name:"Mark Expression"}]},{description:"Show markers (builtin, plugin and per-project ones)",name:"--markers"},{description:"Exit after first num failures or errors",name:"--maxfail",args:[{name:"num"}]},{description:"Run tests from new files first, then the rest of the tests sorted by file mtime",name:["--new-first","--nf"]},{description:"Don't load any conftest.py files",name:"--noconftest"},{description:"Disable header",name:"--no-header"},{description:"Disable summary",name:"--no-summary"},{description:"Override ini option with `option=value` style`",name:["--override-ini","-o"],args:[{name:"Override INI",description:"Ex: `-o xfail_strict=True -o cache_dir=cache"}]},{description:"Early-load given plugin module name or entry point (multi-allowed)",name:"-p",args:[{name:"Plugin name"}]},{description:"Send failed|all info to bpaste.net pastebin service",name:"--pastebin",args:[{name:"mode",suggestions:["failed","all"]}]},{description:"Start the interactive Python debugger on errors or KeyboardInterrupt",name:"--pdb"},{description:"Specify a custom interactive Python debugger for use with --pdb",name:"--pdbcls",args:[{name:"modulename:classname",description:"Ex: --pdbcls=IPython.terminal.debugger:TerminalPdb"}]},{description:"Try to interpret all arguments as python packages",name:"--pyargs"},{description:"Decrease verbosity",name:["--quiet","-q"]},{description:"Show extra test summary info as specified by chars: (f)ailed, (E)rror, (s)kipped, (x)failed, (X)passed, (p)assed, (P)assed with output, (a)ll except passed (p/P), or (A)ll. (w)arnings are enabled by default (see --disable-warnings), 'N' can be used to reset the list. (default: 'fE')",name:"-r",args:[{name:"chars",suggestions:["a","A","E","f","N","p","P","s","w","x","X"]}]},{description:"Define root directory for tests. Can be relative path: 'root_dir', './root_dir', 'root_dir/another_dir/'; absolute path: '/home/user/root_dir'; path with variables:'$HOME/root_dir'",name:"--rootdir",args:[{name:"Root Dir",template:["folders"]}]},{description:"Report the results of xfail tests as if they were not marked",name:"--runxfail"},{description:"Shortcut for --capture=no",name:"-s"},{description:"Only setup fixtures, do not execute tests",name:"--setup-only"},{description:"Show setup of fixtures while executing tests",name:"--setup-show"},{description:"Show what fixtures and tests would be executed but don't execute anything",name:"--setup-plan"},{description:"Controls how captured stdout/stderr/log is shown on failed tests",name:"--show-capture",args:[{name:"Capture method",suggestions:["no","stdout","stderr","log","all"]}]},{description:"Exit on test failure and continue from last failing test next time",name:["--stepwise","--sw"]},{description:"Ignore the first failing test but stop on the next failing test",name:["--stepwise-skip","--sw-skip"]},{description:"Alias to --strict-markers",name:"--strict"},{description:"Any warnings encountered while parsing the `pytest` section of the configuration file raise errors",name:"--strict-config"},{description:"Markers not registered in the `markers` section of the configuration file raise errors",name:"--strict-markers"},{description:"Traceback print mode",name:"--tb",args:[{name:"Traceback print mode",suggestions:["auto","long","short","line","native","no"]}]},{description:"Immediately break when running each test",name:"--trace"},{description:"Trace considerations of conftest.py files",name:"--trace-config"},{description:"Increase verbosity",name:["--verbose","-v"]},{description:"Set verbosity. Default is 0",name:"--verbosity",args:[{name:"Verbosity level"}]},{description:"Display pytest version and information about plugins. When given twice, also display information about plugins",name:["--version","-V"]},{description:"Set which warnings to report, see -W option of python itself",name:["--pythonwarnings","-W"],args:[{name:"Warnings to report"}]}],s={name:e,args:t,options:i};export{t as args,s as default,e as name,i as options};
//# sourceMappingURL=pytest-098fb6d5.js.map
